extensions:
  health_check:
    endpoint: 0.0.0.0:13133
  pprof:
    endpoint: 0.0.0.0:1777
  basicauth/es:
    client_auth:
      username: elastic
      password: password
  
  # Raw sampling buffer extension
  rawsamplingbuffer:
    buffer_size: 10000
    max_entry_size: 1048576  # 1MB
    ttl: 5m
    eviction_policy: lru
  
  # Dynamic sampling configuration extension
  # Fetches sampling rules from Elasticsearch for real-time updates
  samplingconfigextension:
    endpoint: http://localhost:9200
    username: elastic
    password: password
    config_index: .elastic-sampling-config
    poll_interval: 30s  # Check for updates every 30 seconds
    default_sample_rate: 0.0
  
  # Dynamic pipeline extension
  # Loads stream processing pipelines from Elasticsearch
  elasticpipeline:
    source:
      elasticsearch:
        endpoint: http://localhost:9200
        username: elastic
        password: password
        index: .otel-pipeline-config
        cache_duration: 5m
    watcher:
      poll_interval: 3s
      cache_duration: 5m
      filters: []
    pipeline_management:
      namespace: elastic
      enable_health_reporting: true
      health_report_interval: 60s
      startup_timeout: 6s
      shutdown_timeout: 30s
      max_pipelines: 50
      max_components_per_pipeline: 20
      validate_configs: true
      dry_run_mode: false

receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  # == Custom Raw Sampling Processors ==
  
  rawcapture:
    attribute_key: raw.id
    extension_name: rawsamplingbuffer
    skip_on_error: false
  
  samplingdecide:
    # Static fallback: Sample all error logs at 100% rate
    # These settings are used when no dynamic rule matches
    condition: 'severity_text == "ERROR" or attributes["level"] == "error"'
    sample_rate: 0.0
    invert_match: false
    
    # Enable dynamic configuration from Elasticsearch
    # Rules from the extension override static config when matched
    extension_name: samplingconfigextension
  
  rawretriever:
    attribute_key: raw.id
    extension_name: rawsamplingbuffer
    remove_attribute: true
    on_retrieval_error: drop
  
  # == Standard Processors ==
  
  batch:
    timeout: 1s
    send_batch_size: 1024
  
  resource:
    attributes:
      - key: deployment.environment
        value: production
        action: upsert
  
  attributes:
    actions:
      - key: environment
        value: production
        action: insert
  
  # Drop the raw.id attribute from production logs
  attributes/drop_id:
    actions:
      - key: raw.id
        action: delete
  
  transform:
    error_mode: ignore
    log_statements:
      - context: log
        statements:
          - set(attributes["processed"], true)
          # Parse JSON body into structured attributes
          - merge_maps(attributes, ParseJSON(body), "upsert") where IsMatch(body, "^\\{")
          # Extract log level from body if present
          - set(severity_text, attributes["level"]) where attributes["level"] != nil
          # Add processing timestamp
          - set(attributes["processed_at"], Now())

connectors:
  # Stream ingress: Routes to sampling decision
  # Dynamic pipeline support requires full pipeline definitions in Elasticsearch
  routing/stream_ingress:
    default_pipelines: [logs/sampling_decision]
    table:
      - statement: route()
        pipelines: [logs/sampling_decision]
  
  # Output split: Routes to sampling and/or production based on sampling decision
  routing/output_split:
    default_pipelines: [logs/prod, logs/sampling]
    table:
      - statement: route()
        pipelines: [logs/prod, logs/sampling]

exporters:
  debug:
    verbosity: detailed
  
  # Production Elasticsearch exporter
  elasticsearch/prod:
    endpoints:
      - http://localhost:9200
    auth:
      authenticator: basicauth/es
    logs_index: logs
    timeout: 30s
    mapping:
      mode: ecs
  
  # Raw sample exporter - sends to sample API
  rawsample:
    endpoint: http://localhost:9200
    username: elastic
    password: password
    index: logs
    max_batch_size: 100
    timeout: 30s
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 300s

service:
  extensions:
    - health_check
    - pprof
    - basicauth/es
    - rawsamplingbuffer
    - samplingconfigextension
    - elasticpipeline  # Dynamic pipeline extension
  
  pipelines:
    # ========================================================================
    # STAGE 1: INTAKE - Raw Capture & Basic Enrichment
    # ========================================================================
    # Receives incoming telemetry, captures raw data, and performs basic enrichment
    # before routing to stream processing.
    logs/intake:
      receivers:
        - otlp
      processors:
        - rawcapture          # 1. Captures raw log, stores in buffer, adds raw.id
        - resource            # 2. Add deployment.environment
        - attributes          # 3. Add environment tag
      exporters:
        - routing/stream_ingress  # 4. Route to stream processing
    
    # ========================================================================
    # STAGE 2: STREAM PROCESSING
    # ========================================================================
    # This pipeline applies dynamic transforms loaded from Elasticsearch
    # NOTE: For processor-only configs from Elasticsearch, we need a different approach
    # The current architecture requires full pipeline definitions in Elasticsearch
    # including receivers, processors, and exporters
    
    # ========================================================================
    # STAGE 3: SAMPLING DECISION - Evaluate & Decide
    # ========================================================================
    # Receives logs directly from intake (bypassing stream processing for now)
    logs/sampling_decision:
      receivers:
        - routing/stream_ingress  # Receives directly from intake
      processors:
        - transform              # 4. Post-stream processing (JSON parsing, etc.)
        - batch                  # 5. Batch before sampling decision
        - samplingdecide         # 6. Decide what to sample (adds sampled=true attribute)
      exporters:
        - routing/output_split   # 7. Route based on sampling decision
    
    # ========================================================================
    # STAGE 4a: SAMPLING PATH - Raw Sample Export
    # ========================================================================
    # For logs marked as sampled, retrieve the original raw data and send to
    # the raw sample exporter for storage in the sampling system.
    logs/sampling:
      receivers:
        - routing/output_split   # Receives sampled logs
      processors:
        - rawretriever           # Retrieve original raw log from buffer
      exporters:
        - rawsample              # Send raw sample to API
        - debug                  # Debug output
    
    # ========================================================================
    # STAGE 4b: PRODUCTION PATH - Processed Export
    # ========================================================================
    # All logs (sampled and non-sampled) go to production with processed data.
    # The raw.id attribute is removed before export.
    logs/prod:
      receivers:
        - routing/output_split   # Receives all logs
      processors:
        - attributes/drop_id     # Remove raw.id attribute
      exporters:
        - elasticsearch/prod     # Send to production Elasticsearch
        - debug                  # Debug output

  telemetry:
    logs:
      level: info
