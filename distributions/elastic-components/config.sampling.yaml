extensions:
  health_check:
    endpoint: 0.0.0.0:13133
  pprof:
    endpoint: 0.0.0.0:1777
  basicauth/es:
    client_auth:
      username: elastic
      password: password
  
  # Raw sampling buffer extension
  rawsamplingbuffer:
    buffer_size: 10000
    max_entry_size: 1048576  # 1MB
    ttl: 5m
    eviction_policy: lru

receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  # == Custom Raw Sampling Processors ==
  
  rawcapture:
    attribute_key: raw.id
    extension_name: rawsamplingbuffer
    skip_on_error: false
  
  samplingdecide:
    # Example: Sample all error logs at 10% rate
    condition: 'severity_text == "ERROR" or attributes["level"] == "error"'
    sample_rate: 1
    invert_match: false
  
  rawretriever:
    attribute_key: raw.id
    extension_name: rawsamplingbuffer
    remove_attribute: true
    on_retrieval_error: drop
  
  # == Standard Processors ==
  
  batch:
    timeout: 1s
    send_batch_size: 1024
  
  resource:
    attributes:
      - key: deployment.environment
        value: production
        action: upsert
  
  attributes:
    actions:
      - key: environment
        value: production
        action: insert
  
  # Drop the raw.id attribute from production logs
  attributes/drop_id:
    actions:
      - key: raw.id
        action: delete
  
  transform:
    error_mode: ignore
    log_statements:
      - context: log
        statements:
          - set(attributes["processed"], true)
          # Parse JSON body into structured attributes
          - merge_maps(attributes, ParseJSON(body), "upsert") where IsMatch(body, "^\\{")
          # Extract log level from body if present
          - set(severity_text, attributes["level"]) where attributes["level"] != nil
          # Add processing timestamp
          - set(attributes["processed_at"], Now())

connectors:
  routing:
    default_pipelines: []
    table:
      - statement: route()
        pipelines: [logs/prod, logs/sampling]

exporters:
  debug:
    verbosity: detailed
  
  # Production Elasticsearch exporter
  elasticsearch/prod:
    endpoints:
      - http://localhost:9200
    auth:
      authenticator: basicauth/es
    logs_index: logs-production
    timeout: 30s
    mapping:
      mode: ecs
  
  # Raw logs Elasticsearch exporter
  elasticsearch/raw:
    endpoints:
      - http://localhost:9200
    auth:
      authenticator: basicauth/es
    logs_index: logs-rawsamples
    timeout: 30s
    mapping:
      mode: ecs

service:
  extensions:
    - health_check
    - pprof
    - basicauth/es
    - rawsamplingbuffer
  
  pipelines:
    # == 1. Intake Pipeline ==
    logs/intake:
      receivers:
        - otlp
      processors:
        - rawcapture      # 1. Captures raw log, stores in buffer, adds raw.id
        - resource        # 2. Standard processing
        - attributes
        - transform
        - batch           # 3. Batch before routing
      exporters:
        - routing         # 4. Fork to prod and sampling pipelines
    
    # == 2. Production Pipeline ==
    logs/prod:
      receivers:
        - routing         # Receives from routing connector
      processors:
        - attributes/drop_id  # Remove raw.id attribute
      exporters:
        - elasticsearch/prod
        - debug
    
    # == 3. Sampling Pipeline (Raw Logs) ==
    logs/sampling:
      receivers:
        - routing         # Receives from routing connector
      processors:
        - samplingdecide  # Evaluate condition, drop non-matches
        - rawretriever    # Retrieve raw log from buffer
      exporters:
        - debug           # Just debug for now
        - elasticsearch/raw

  telemetry:
    logs:
      level: info
