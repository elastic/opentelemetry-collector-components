# Dynamic Routing Connector

The Dynamic Routing Connector is an OpenTelemetry Collector connector that routes telemetry data (traces, logs, and metrics) to different pipelines based on the estimated cardinality of unique combinations of configured metadata keys. It uses the HyperLogLog algorithm to efficiently estimate cardinality without storing all unique identifiers, making it memory-efficient even at scale. The metadata keys you configure determine what type of cardinality is being measured—whether that's unique connections, unique services, unique pods, or any other combination of metadata attributes.

<!-- status autogenerated section -->
| Status        |           |
| ------------- |-----------|
| Distributions | [] |
| Warnings      | [Statefulness](#warnings) |
| Issues        | [![Open issues](https://img.shields.io/github/issues-search/elastic/opentelemetry-collector-components?query=is%3Aissue%20is%3Aopen%20label%3Aconnector%2Fdynamicrouting%20&label=open&color=orange&logo=opentelemetry)](https://github.com/elastic/opentelemetry-collector-components/issues?q=is%3Aopen+is%3Aissue+label%3Aconnector%2Fdynamicrouting) [![Closed issues](https://img.shields.io/github/issues-search/elastic/opentelemetry-collector-components?query=is%3Aissue%20is%3Aclosed%20label%3Aconnector%2Fdynamicrouting%20&label=closed&color=blue&logo=opentelemetry)](https://github.com/elastic/opentelemetry-collector-components/issues?q=is%3Aclosed+is%3Aissue+label%3Aconnector%2Fdynamicrouting) |
| Code coverage | [![codecov](https://codecov.io/github/elastic/opentelemetry-collector-components/graph/main/badge.svg?component=connector_dynamicrouting)](https://app.codecov.io/gh/elastic/opentelemetry-collector-components/tree/main/?components%5B0%5D=connector_dynamicrouting&displayType=list) |

[development]: https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/component-stability.md#development

## Supported Pipeline Types

| [Exporter Pipeline Type] | [Receiver Pipeline Type] | [Stability Level] |
| ------------------------ | ------------------------ | ----------------- |
| logs | logs | [development] |
| metrics | metrics | [development] |
| traces | traces | [development] |

[Exporter Pipeline Type]: https://github.com/open-telemetry/opentelemetry-collector/blob/main/connector/README.md#exporter-pipeline-type
[Receiver Pipeline Type]: https://github.com/open-telemetry/opentelemetry-collector/blob/main/connector/README.md#receiver-pipeline-type
[Stability Level]: https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/component-stability.md#stability-levels
<!-- end autogenerated section -->

## Overview

The Dynamic Routing Connector enables intelligent, data-driven routing of telemetry signals to different processing pipelines based on the observed cardinality of unique combinations of metadata keys. The connector estimates how many unique combinations exist for a given primary key (e.g., tenant ID) based on the configured metadata keys. This is particularly useful in multi-tenant environments or scenarios where different workloads require different processing strategies based on their cardinality characteristics.

### The Problem It Solves

Traditional OpenTelemetry Collector configurations use static routing rules that are defined at configuration time. This approach has limitations:

1. **Static Configuration**: Routing decisions are fixed and cannot adapt to changing traffic patterns
2. **One-Size-Fits-All**: All data flows through the same pipeline regardless of volume or connection patterns
3. **Inefficient Resource Usage**: High-cardinality tenants may need different batching, sampling, or processing strategies than low-cardinality ones
4. **Manual Tuning Required**: Operators must manually configure routing rules based on assumptions about traffic patterns

The Dynamic Routing Connector fills this gap by providing **adaptive, cardinality-based routing** that automatically adjusts pipeline selection based on observed cardinality patterns derived from configured metadata keys.

### How It Works

The connector uses the following approach:

1. **Metadata Extraction**: For each incoming telemetry signal, the connector extracts metadata from the client context using the configured `primary_metadata_key` and `metadata_keys`. The `primary_metadata_key` is used to partition cardinality estimates (e.g., per tenant, per service), while `metadata_keys` define what unique combinations are being counted.

2. **Cardinality Estimation**: The connector uses the HyperLogLog algorithm to estimate the number of unique combinations of the configured `metadata_keys` for each value of the `primary_metadata_key`. For example:
   - If `metadata_keys` are `["x-forwarded-for", "user-agent"]`, it estimates unique connection combinations
   - If `metadata_keys` are `["pod.name", "container.id"]`, it estimates unique pod/container combinations
   - If `metadata_keys` are `["service.name", "deployment.name"]`, it estimates unique service/deployment combinations

3. **Threshold-Based Routing**: Based on the estimated cardinality for each primary key value, the connector routes data to different pipelines defined by threshold boundaries. For example:
   - Low cardinality (0-10 unique combinations) → Pipeline A
   - Medium cardinality (11-100 unique combinations) → Pipeline B  
   - High cardinality (101+ unique combinations) → Pipeline C

4. **Periodic Re-evaluation**: At configurable intervals, the connector re-evaluates routing decisions based on the most recent cardinality estimates, allowing it to adapt to changing patterns.

5. **Memory Efficiency**: The HyperLogLog algorithm provides accurate cardinality estimates with minimal memory overhead, making it suitable for high-throughput scenarios.

## Configuration

### Basic Configuration

```yaml
connectors:
  dynamicrouting:
    primary_metadata_key: "x-tenant-id"
    metadata_keys:
      - "x-forwarded-for"
      - "user-agent"
    thresholds: [10, 100, 500]
    pipelines:
      - ["traces/low_cardinality"]
      - ["traces/medium_cardinality"]
      - ["traces/high_cardinality"]
      - ["traces/very_high_cardinality"]
    default_pipelines: ["traces/default"]
    evalaution_interval: 30s
```

### Configuration Fields

| Field | Type | Description | Required |
|-------|------|-------------|----------|
| `primary_metadata_key` | string | The primary metadata key used to partition cardinality estimates (e.g., tenant ID, service name). Each unique value of this key will have its own cardinality estimate. | Yes |
| `metadata_keys` | []string | Metadata keys used to define unique combinations for cardinality estimation. The connector counts how many unique combinations of these keys exist for each value of `primary_metadata_key`. The choice of keys determines what type of cardinality is measured (e.g., unique connections, unique pods, unique deployments). | No |
| `thresholds` | []int | Array of threshold values in ascending order. Defines the boundaries for routing decisions. Must have `len(pipelines) - 1` elements. | Yes |
| `pipelines` | [][]pipeline.ID | Array of pipeline ID arrays, one for each threshold bucket. The number of pipeline arrays must be `len(thresholds) + 1` (to include the +inf bucket). | Yes |
| `default_pipelines` | []pipeline.ID | Pipelines to use when the primary metadata key is missing from the client context. | No |
| `evalaution_interval` | duration | How often to re-evaluate routing decisions based on new cardinality estimates. Default: 30s | No |

### Configuration Rules

- `thresholds` must be in ascending order and contain unique values
- The number of `pipelines` must equal `len(thresholds) + 1` (one for each threshold interval plus one for values above the highest threshold)
- At least one pipeline must be defined

### Routing Logic

The connector routes data based on the estimated cardinality for the primary key:

- If `primary_metadata_key` is missing → routes to `default_pipelines`
- If cardinality < `thresholds[0]` → routes to `pipelines[0]`
- If `thresholds[i]` ≤ cardinality < `thresholds[i+1]` → routes to `pipelines[i+1]`
- If cardinality ≥ `thresholds[len(thresholds)-1]` → routes to `pipelines[len(pipelines)-1]`

## Use Cases

### Dynamic Batching Based on Cardinality

One of the most powerful use cases for the Dynamic Routing Connector is implementing dynamic batching strategies based on cardinality. By configuring `metadata_keys` to represent unique connections (e.g., source IP and user agent), you can route tenants with different connection volumes to different batching pipelines.

**Scenario**: You're operating a multi-tenant observability platform where different tenants have vastly different cardinality patterns. Some tenants have a few unique combinations (e.g., few connections, few pods, few services), while others have many.

**Problem**: Using a single batching configuration for all tenants leads to:
- **Low-cardinality tenants**: Small batches that are inefficient and increase overhead
- **High-cardinality tenants**: Large batches that may cause memory pressure and latency spikes

**Solution**: Use the Dynamic Routing Connector to route tenants to different pipelines with optimized batching configurations based on their cardinality:

```yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317

connectors:
  dynamicrouting:
    primary_metadata_key: "x-tenant-id"
    metadata_keys:
      - "x-forwarded-for"
      - "user-agent"
    thresholds: [10, 50, 200]
    pipelines:
      # 0-10 unique connections: Small batches, frequent flush
      - ["traces/small_batch"]
      # 11-50 unique connections: Medium batches
      - ["traces/medium_batch"]
      # 51-200 unique connections: Large batches
      - ["traces/large_batch"]
      # 200+ unique connections: Very large batches, aggressive batching
      - ["traces/xlarge_batch"]
    default_pipelines: ["traces/default"]
    evalaution_interval: 30s

processors:
  batch/small:
    timeout: 1s
    send_batch_size: 100
    send_batch_max_size: 200
  
  batch/medium:
    timeout: 5s
    send_batch_size: 500
    send_batch_max_size: 1000
  
  batch/large:
    timeout: 10s
    send_batch_size: 2000
    send_batch_max_size: 5000
  
  batch/xlarge:
    timeout: 30s
    send_batch_size: 5000
    send_batch_max_size: 10000

exporters:
  otlp/elastic:
    endpoint: https://elastic-cloud-endpoint:443
    headers:
      Authorization: "Bearer ${ELASTIC_API_KEY}"

service:
  pipelines:
    traces:
      receivers: [otlp]
      connectors: [dynamicrouting]
  
    traces/small_batch:
      processors: [batch/small]
      exporters: [otlp/elastic]
  
    traces/medium_batch:
      processors: [batch/medium]
      exporters: [otlp/elastic]
  
    traces/large_batch:
      processors: [batch/large]
      exporters: [otlp/elastic]
  
    traces/xlarge_batch:
      processors: [batch/xlarge]
      exporters: [otlp/elastic]
  
    traces/default:
      processors: [batch/medium]
      exporters: [otlp/elastic]
```

**How It Works**:

1. **Cardinality Tracking**: For each tenant (identified by `x-tenant-id`), the connector tracks unique combinations of the configured `metadata_keys` (`x-forwarded-for` and `user-agent` in this example). This measures the cardinality of unique connection combinations per tenant.

2. **Cardinality Estimation**: Using HyperLogLog, the connector estimates how many unique combinations each tenant has without storing all identifiers. In this case, it estimates unique connection combinations.

3. **Dynamic Routing**: Based on the estimated cardinality:
   - **Tenant A** (5 unique connection combinations) → `traces/small_batch` pipeline with 1s timeout and 100-item batches
   - **Tenant B** (25 unique connection combinations) → `traces/medium_batch` pipeline with 5s timeout and 500-item batches
   - **Tenant C** (150 unique connection combinations) → `traces/large_batch` pipeline with 10s timeout and 2000-item batches
   - **Tenant D** (500 unique connection combinations) → `traces/xlarge_batch` pipeline with 30s timeout and 5000-item batches

4. **Adaptive Behavior**: Every 30 seconds, the connector re-evaluates routing decisions. If Tenant A's cardinality grows to 15, it automatically switches to the `traces/medium_batch` pipeline.

**Benefits**:
- **Optimized Throughput**: High-cardinality tenants benefit from larger batches, reducing overhead
- **Lower Latency**: Low-cardinality tenants get faster processing with smaller batches
- **Resource Efficiency**: Memory and CPU usage are optimized per tenant workload
- **Automatic Adaptation**: No manual intervention needed as traffic patterns change

### Other Use Cases

The connector is flexible—the metadata keys you configure determine what type of cardinality is measured:

- **Unique Services per Tenant**: Set `primary_metadata_key: "x-tenant-id"` and `metadata_keys: ["service.name"]` to route based on the number of unique services per tenant
- **Unique Pods per Service**: Set `primary_metadata_key: "service.name"` and `metadata_keys: ["pod.name", "container.id"]` to route based on the number of unique pod/container combinations per service
- **Unique Deployments per Namespace**: Set `primary_metadata_key: "namespace"` and `metadata_keys: ["deployment.name"]` to route based on deployment cardinality
- **Sampling Strategies**: Route high-cardinality entities to pipelines with more aggressive sampling
- **Processing Pipelines**: Apply different processors (filtering, transformation) based on cardinality patterns
- **Export Destinations**: Route different tiers to different exporters or backends based on their cardinality characteristics
- **Quality of Service**: Provide different QoS guarantees based on observed cardinality

## Implementation Details

### HyperLogLog Algorithm

The connector uses the HyperLogLog probabilistic data structure to estimate cardinality. This provides:

- **Memory Efficiency**: Constant memory usage regardless of the number of unique combinations being tracked
- **Accuracy**: Typical error rate of ~1% for cardinality estimation
- **Performance**: O(1) insertion and estimation operations

### Evaluation Interval

The `evalaution_interval` determines how frequently routing decisions are updated:
- **Shorter intervals**: More responsive to changes but higher CPU usage
- **Longer intervals**: More stable routing but slower adaptation to traffic changes
- **Recommended**: 30-60 seconds for most use cases

## Warnings

### Statefulness

This connector maintains state (HyperLogLog sketches) in memory. Important considerations:

- **Memory Usage**: Memory usage scales with the number of unique primary key values, not the total number of unique combinations being tracked
- **State Loss**: State is lost on collector restart. Routing decisions will rebuild over the evaluation interval
- **High Cardinality Primary Keys**: If you have many unique values for `primary_metadata_key`, memory usage will increase proportionally

### Metadata Requirements

- The connector requires client metadata to be set in the context. Ensure your receivers/proxies propagate metadata appropriately
- Missing `primary_metadata_key` will route to `default_pipelines`
- Missing `metadata_keys` will still work, but the cardinality estimation will be based on the primary key alone (which may not provide meaningful cardinality measurements)
- The choice of `metadata_keys` determines what type of cardinality is being measured—choose keys that represent the unique combinations you want to track

## Troubleshooting

### All Data Routes to Default Pipeline

- **Check**: Verify that `primary_metadata_key` is present in client metadata
- **Solution**: Ensure your receiver or proxy is setting the metadata in the context

### Routing Not Updating

- **Check**: Verify `evalaution_interval` is not too long
- **Solution**: Reduce the interval or manually trigger evaluation (requires collector restart)

### High Memory Usage

- **Check**: Number of unique values for `primary_metadata_key`
- **Solution**: Consider using a more selective primary key or increasing evaluation interval to reduce state accumulation

## Contributing

Contributions are welcome! Please see the main repository [contributing guidelines](https://github.com/elastic/opentelemetry-collector-components/blob/main/CONTRIBUTING.md) for details.
